## LLM‑assisted evaluation (Qwen3)

This script runs an LLM‑assisted evaluation to decide which target classes are mentioned in a model’s caption (audio/video/AV). It uses a Qwen3 chat model to compare, per sample, the caption generated by an open‑source foundation model against the ground‑truth class list, and returns for each class a yes/no decision and a 0–5 relevance score.

### What it does
- **Input**: a CSV of model captions per `video_id` and a target labels CSV.
- **LLM evaluator**: Qwen3 receives a prompt that compares the caption with the list of target classes and returns a Python dict mapping each class to `{pred: "yes"|"no", score: 0..5}`.
- **Output**: the predictions CSV is updated in‑place (or to a copy) by filling the `suggestions` column with the evaluator’s dict string.


Internally, the evaluator also uses a system prompt to enforce the strict output format (per‑class yes/no and integer score 0–5).

## File formats

### Predictions CSV (input and output)
Path convention: `experiments/gpt-evaluation/csv/{a|v|av}/predictions_*.csv`

- **video_id**: string with `.mp4` suffix (e.g., `glLQrEijrKg_000300.mp4`).
- **suggestions**: string; use `[]` for unevaluated rows. After evaluation it becomes a Python dict string mapping class → `{pred, score}`.
- **response**: caption generated by the foundation model for that sample (short answer).

Example row before evaluation:

```text
video_id,suggestions,response
glLQrEijrKg_000300.mp4,[],Playing hammond organ
```

Example row after evaluation:

```text
video_id,suggestions,response
glLQrEijrKg_000300.mp4,"{""male speech, man speaking"": {""pred"": ""yes"", ""score"": 5}, ""playing hammond organ"": {""pred"": ""yes"", ""score"": 3}}",Playing hammond organ
```

**How to generate the `response` column (for reproducibility):**
- For each sample, the open‑source foundation models are prompted depending on the input modality:
  - **A**: “What actions are being performed in this audio, explain all sounds and actions in the audio? Please provide a short answer.”
  - **V/AV**: “What actions are being performed in this video, explain all sounds and actions in the video? Please provide a short answer.”
- The generated caption (audio/video description) should be stored in the csv under the collumn `response`

### Target labels CSV (input)
Default: `vggsounder/data/vggsounder+background-music.csv`

Columns:
- **video_id**: string without suffix (e.g., `glLQrEijrKg_000300`).
- **label**: a single class name per row (dataset class taxonomy).
- **modality, background_music, static_image, voice_over**: metadata columns (not used by the evaluator logic).

Notes:
- The file is multi‑row per video; the script groups by `video_id` and builds a Python list of class names per sample.
- The evaluator internally appends `.mp4` to the grouped `video_id`s to match the predictions CSV.

## Usage

### Quick start (recommended)
Install deps (Python 3.10–3.12):

```bash
pip install -r experiments/gpt-evaluation/requirements.txt
```

Run the helper script (sets the proper question based on modality):

```bash
cd experiments/gpt-evaluation
bash local.sh <model_name> <modality> [batch_size]
```

Notes:
- `modality` is one of `a`, `v`, `av`. The script reads `./csv/<modality>/predictions_video_llama_2.csv` by default.
- `local.sh` currently passes `--model Qwen/Qwen3-32B`; edit it if you want a different HF model.

### Direct CLI
You can call the evaluator directly for full control:

```bash
python experiments/gpt-evaluation/evaluate_model.py \
  --model Qwen/Qwen3-32B \
  --question "placeholder" \
  --prediction_csv experiments/gpt-evaluation/csv/a/predictions_video_llama_2.csv \
  --target_csv vggsounder/data/vggsounder+background-music.csv \
  --backend transformers \
  --batch_size 8
```

Flags:
- **--model**: HF model id, e.g., `Qwen/Qwen3-32B` (uses 8‑bit loading by default).
- **--backend**: `transformers` (recommended). Other backends are not enabled here.
- **--prediction_csv**: path to your predictions file.
- **--target_csv**: path to the labels CSV.
- **--batch_size**: evaluator batch size.
- **--question**: required CLI flag; the script recomputes the proper question from the CSV modality, so the value is ignored.
- **--rerun**: if set, re‑evaluates all rows (overwrites `suggestions`).
- **--make_copy**: if set, writes to a `*_copy.csv` instead of overwriting the input.

Resume behavior:
- If `--rerun` is not set, the script finds the first row where `suggestions == "[]"` and resumes from there.

## Tips
- GPU is strongly recommended. The evaluator loads models via `transformers` with 8‑bit quantization (`bitsandbytes`).
- Ensure your `video_id`s line up: predictions must use `.mp4`, targets must not. The script harmonizes them internally.


